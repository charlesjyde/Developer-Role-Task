# Developer-Role-Task

## Scenario 1 - Steps to Deploy AI Assistant for Fintech Startup

### Scope Summary
AI assistant to support customer service representatives. The assistant will be trained on company's internal documents including customer data, regulatory compliance information, and internal company policies. It will strictly comply with the compliance information and company policies of the compnay and ensure strict data privacy compliance when communicating with external APIs.

### Model Selection 
I will first research existing AI models and Infrastructure that will meet this requirement. From my existing knowledge and experience developing AI Assistants for small companies, I will consider the following:

If cost of maintaining the AI Assistant is not an issue
1. Azure OpenAI Service ( using GPT-4-turbo)
2. Open AI GPT-4- turbo

If cost of maintaing the AI Assistant should be low, I will consider open-source models:
1. Mistral AI
2. Llama 2 (Llama 3 will be released soon and it will be multimodal and high performance)

### Design
For the purposes of this task, I will select Open AI GPT-4-turbo and LangChain for the following:
1. Langchain for Retrieval-Augmented Generation (RAG) 
2. GPT-4-Turbo for Function Calling -
    - add a reviwer function that will check that prompts and responses comply with compliance information and company policies in the database.
    - add a function call that will query database to perform calculations on the fly. Some of the queries will be hard coded and some will be generated by the AI on the fly
3. Superbase or Pinecone for vector storage

NB: For RAG and Function calling, we can use Assistant API if the customer is compfortable uploading their documents to OpenAI. This actually simplifies the process but will not be suitable for complex and large documents.

### App Flow Diagram
#### Load Documents in Vector Store
1. Information Source: database that holds the company information that the customer service representatives use
2. Splitter - Langchain Tool to split documents into chunks of data
3. OpenAI Embeddings: to create vectors from each chunk
4. Superbase: store the chucks in the vetor store

#### Process Flow
![Alt Text for Image](/charlesjyde/diagram.drawio.png)
1. User Input: this is the users question or prompt
2. Converstaion Memory: save users question to converstion store
3. OpenAI GPT-4-turbo  - format users question to a concise and precise question
4. OpenAI Embeddings Model - pass the formatted question to create vectors from it
5. Send the vector store to superbase vector store to get back chunck(s) with the nearest match which are most likely to contain the answer to the users question
6. Pass the orginal user input, converstation history and returned chunk to an OpenAI GPT-4-turbo to get the accurate answer

### Development
1. Setup a new project using Django (Backend) and React (Frontend)
2. 




